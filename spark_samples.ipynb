{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_samples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0occcUzLOEgmPDclaKbUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gunarevuri/Eng_times/blob/master/spark_samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHp7XtHL0_vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e907821-13da-42ec-b2ce-d79ff692fa1c"
      },
      "source": [
        "!apt-get set up openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Invalid operation set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af5puhRF1Awg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsyBwi5S1MqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b58cac1d-af11-4001-ab09-0eccc99e06b1"
      },
      "source": [
        "!tar xf spark-2.3.2-bin-hadoop2.7.tgz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-2.3.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZTGraQT1QbN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "5f4705ed-a4c5-4b00-9a1d-b6986b7edfde"
      },
      "source": [
        "import findspark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ff073c74b5db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIxUvjfU1bEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRjZH4W01ilG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qbIxdyL1q-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3cffa782-af45-4da3-d90f-dfc8aeb51e7f"
      },
      "source": [
        "!tar xf spark-2.3.2-bin-hadoop2.7.tgz"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-2.3.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i319BYQg1uPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "54d39d86-aedf-4d1f-bfb4-f7c520db871b"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/b0/bf9020b56492281b9c9d8aae8f44ff51e1bc91b3ef5a884385cb4e389a40/pyspark-3.0.0.tar.gz (204.7MB)\n",
            "\u001b[K     |████████████████████████████████| 204.7MB 66kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 37.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=b51769dc6e8f9eb49f96d0ddd38d4af0d137a88cc24bc2f3e58b55386b30f2a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/27/4d/ddacf7143f8d5b76c45c61ee2e43d9f8492fc5a8e78ebd7d37\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvldETIl14WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnyGy6F42HV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "0312ea0b-08fd-4bf3-d8f8-3f8446877418"
      },
      "source": [
        "./bin/spark\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-23026dc19f0b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ./bin/spark\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk04OTW22RUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HFX7Wjr3poi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89a24afa-e22e-4d41-a575-4971371b39e8"
      },
      "source": [
        "print(os.getcwd())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDeI2jgb3spt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "5bbf077c-6389-4054-cb30-4fc41da75c2a"
      },
      "source": [
        "print(os.dir(os.getcwd()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ab657d410fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'dir'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXg85TlO3xSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "0a4bed07-bd9c-4187-f819-bd319207363c"
      },
      "source": [
        "for root, dir, file in os.walk(os.getcwd()):\n",
        "  print(root)\n",
        "  print(dir)\n",
        "  print(file)\n",
        "  print(\"finifsh----\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "['.config', 'sample_data']\n",
            "[]\n",
            "finifsh----\n",
            "/content/.config\n",
            "['configurations', 'logs']\n",
            "['.last_opt_in_prompt.yaml', 'active_config', 'config_sentinel', '.last_survey_prompt.yaml', '.last_update_check.json', 'gce', '.metricsUUID']\n",
            "finifsh----\n",
            "/content/.config/configurations\n",
            "[]\n",
            "['config_default']\n",
            "finifsh----\n",
            "/content/.config/logs\n",
            "['2020.07.10']\n",
            "[]\n",
            "finifsh----\n",
            "/content/.config/logs/2020.07.10\n",
            "[]\n",
            "['16.29.12.848063.log', '16.28.57.265459.log', '16.28.19.621895.log', '16.28.38.231204.log', '16.28.52.396365.log', '16.29.12.134415.log']\n",
            "finifsh----\n",
            "/content/sample_data\n",
            "[]\n",
            "['anscombe.json', 'README.md', 'california_housing_test.csv', 'mnist_test.csv', 'mnist_train_small.csv', 'california_housing_train.csv']\n",
            "finifsh----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_ifh1ci3_V1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "280f2922-3119-4207-85b9-9e4e6ee0e27a"
      },
      "source": [
        "import re\n",
        "for root, dir, file in os.walk(os.getcwd()):\n",
        "\n",
        "  if file == 'mnist_train_small.csv':\n",
        "    print(os.getcwd(file))\n",
        "  print(\"finifsh----\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finifsh----\n",
            "finifsh----\n",
            "finifsh----\n",
            "finifsh----\n",
            "finifsh----\n",
            "finifsh----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRzuTGRx4rah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "541f6083-3170-45a3-f4e2-8f5e8cfe02c3"
      },
      "source": [
        "print(os.path.abspath(os.getcwd()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM__AyJh5B0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = '/content/sample_data/README.md'"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wfmn09l5YYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULfPhn6h5hhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext(\"local\", \"first app\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNtYJPtK5r0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "6f4b1ed7-9b89-4381-ab5e-5f85e1d8baaa"
      },
      "source": [
        "logData = sc.read.txt(file)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ab6d54cd40a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'read'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2j9auDu53ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b59f3767-749a-402d-aaa7-0f5695189e33"
      },
      "source": [
        "sc.textFile(file)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IHqa7X66DhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdata = sc.textFile(file)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9au7zgC6SjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Print Num of 'a' in logData\n",
        "numAs = logdata.filter(lambda s : 'a' in s).count()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1wsFMfQ6p3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6557471f-a652-455c-f1a7-18e84453bed0"
      },
      "source": [
        "numAs"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0nh4zyq6rv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numBs = logdata.filter(lambda s : 'b' in s).count()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8eUdZYp67gK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d42c758-f2c8-4df2-efaf-493aa64176e0"
      },
      "source": [
        "numBs"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq-5FzCr68r2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "f692683d-a768-438c-aacf-03de976fba96"
      },
      "source": [
        "class pysparkRDD (\n",
        "   jrdd, \n",
        "   ctx, \n",
        "   jrdd_deserializer = AutoBatchedSerializer(PickleSerializer()))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-03050d61c660>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    jrdd_deserializer = AutoBatchedSerializer(PickleSerializer()))\u001b[0m\n\u001b[0m                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsopgEKE7nvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = sc.parallelize (\n",
        "   [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"akka\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\"]\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu8HKd1d716P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "519d1426-d8b2-47d3-c6da-a7bad604010b"
      },
      "source": [
        "count = words.count()\n",
        "count\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abmoSFZ18AQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "d4b27203-b4c9-4027-91af-c1a22387654c"
      },
      "source": [
        "print(hostname)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-ef0104e9b3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'hostname' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLKR0ScZ8IXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "009c26d0-af40-4d17-f013-e562f58f2869"
      },
      "source": [
        "!curl ipecho.net/plain"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35.203.186.116"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYc-nR8I8S7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d73cc420-ef3d-4ebf-c301-a11c20102575"
      },
      "source": [
        " \n",
        "import socket\n",
        "print(socket.gethostname())\n",
        "names = os.uname()\n",
        "print(names)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e64c29801fcf\n",
            "posix.uname_result(sysname='Linux', nodename='e64c29801fcf', release='4.19.104+', version='#1 SMP Wed Feb 19 05:26:34 PST 2020', machine='x86_64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34vz92F88h7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "993e4b88-f71f-4b15-afe2-e9e57bc25d8d"
      },
      "source": [
        "count_collect = words.collect()\n",
        "count_collect"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scala',\n",
              " 'java',\n",
              " 'hadoop',\n",
              " 'spark',\n",
              " 'akka',\n",
              " 'spark vs hadoop',\n",
              " 'pyspark',\n",
              " 'pyspark and spark']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvBeqI5n9WR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "570d8962-8634-4cd1-a044-8169ab3f0d22"
      },
      "source": [
        "output = words.filter(lambda s: 'spark' in s).collect()\n",
        "print(output)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN4j8A-h-D9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "bacd81bf-58de-473f-fb9a-48ab7dbd957e"
      },
      "source": [
        "# Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks. The following code block has the details of a Broadcast class for PySpark.\n",
        "class pyspark(Broadcast):\n",
        "   sc = None, \n",
        "   value = None, \n",
        "   pickle_registry = None, \n",
        "   path = None\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-1f861299ee3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks. The following code block has the details of a Broadcast class for PySpark.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBroadcast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mpickle_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Broadcast' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqN41AFf_2uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_broadcast = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR3K0xRhANy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = words_broadcast.value"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVgjGhuWAUyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a62c8997-cc59-4696-d9f9-c994efab6503"
      },
      "source": [
        "data"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scala', 'java', 'hadoop', 'spark', 'akka']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG1uKw9BAVca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ee523c1-dd57-45bf-d163-041b1559bcd3"
      },
      "source": [
        "type(data)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmNUhb-CAYKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The following example shows how to use an Accumulator variable. An Accumulator variable has an attribute called value that is similar to what a broadcast variable has. It stores the data and is used to return the accumulator's value, but usable only in a driver program."
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSe07d3iBJhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ac_variable = sc.accumulator(10)\n",
        "def f(x):\n",
        "  global ac_variable\n",
        "  ac_variable += 1\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAy6ToZWBv7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd_list = sc.parallelize([23,32,34,6,3,9])\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS_ukM4XB5iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd_list.foreach(f) \n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbR82hesB7KE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70884954-9ce4-4453-9631-f1884f66d66c"
      },
      "source": [
        "final = ac_variable.value\n",
        "print(final)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24cKephaCRfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e25e2fa-0966-4089-b51f-ebf0326a2fe7"
      },
      "source": [
        "#you can upload your files using sc.addFile (sc is your default SparkContext) and get the path on a worker using SparkFiles.get. Thus, SparkFiles resolve the paths to files added through SparkContext.addFile().\n",
        "#SparkFiles contain the following classmethods −\n",
        "#get(filename)\n",
        "#getrootdirectory()\n",
        "#Let us understand them in detail.\n",
        "from pyspark import SparkFiles\n",
        "findinstance = '/content/sample_data/mnist_test.csv'\n",
        "sc.addFile(findinstance)\n",
        "print(SparkFiles.get(findinstance))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/sample_data/mnist_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IPAann4F_-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}